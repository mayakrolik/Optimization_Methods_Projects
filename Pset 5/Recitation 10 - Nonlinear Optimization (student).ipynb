{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Optimization\n",
    "\n",
    "by Sean Lo (seanlo@mit.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pkg;\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"LinearAlgebra\")\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"Random\")\n",
    "# Pkg.add(\"NLsolve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, CSV, DataFrames, Random\n",
    "using NLsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choices in step size for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the impact of step size on gradient descent for different problems! The different step size choices we will look at are:\n",
    "- constant step size\n",
    "- exact line search, $\\alpha_k = \\argmin_{\\alpha} f(\\bm{x}^k - \\alpha \\bm{d}^k)$\n",
    "- and backtracking line search, where you start with a \"big\" $\\bar{\\alpha}$ and decrement it until $f(\\bm{x}^k - \\bar{\\alpha} \\bm{d}^k)$ is \"small enough\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exact line search v.s. constant step size, for quadratic optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider the problem \n",
    "$$\\min_{x_1, x_2} f(x_1, x_2) := 5x_1^2 + x_2^2 + 4 x_1 x_2 - 14 x_1 - 6 x_2 + 20$$\n",
    "This is a quadratic in $\\bm{x} = (x_1, x_2)$, since we can write $f$ as:\n",
    "$$f(\\bm{x}) = \\frac{1}{2} \\bm{x}^\\top  \\bm{Q} \\bm{x} - \\bm{c}^\\top \\bm{x} + b$$\n",
    "with $\\bm{Q} = \\begin{pmatrix} 10 & 4 \\\\ 4 & 2 \\end{pmatrix}$, $\\bm{c} = \\begin{pmatrix} 14 \\\\ 6 \\end{pmatrix}$, and $b = 20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write functions to compute the cost $f(\\bm{x})$ and gradient $\\nabla f(\\bm{x})$ as a function of $\\bm{x}$:\n",
    "\\begin{align}\n",
    "    \\bm{d}^k = - \\nabla f(\\bm{x}^k) = \n",
    "    - \\begin{pmatrix}\n",
    "        10 x_1^k + 4 x_2^k - 14 \n",
    "        \\\\\n",
    "         2 x_2^k + 4 x_1^k - 6\n",
    "    \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function_quadratic(x::Vector)\n",
    "    return 5 * x[1]^2 + x[2]^2 + 4 * x[1] * x[2] - 14 * x[1] - 6 * x[2] + 20\n",
    "end\n",
    "\n",
    "function gradient_quadratic(x::Vector)\n",
    "    return [\n",
    "        10 * x[1] + 4 * x[2] - 14,\n",
    "         2 * x[2] + 4 * x[1] - 6,\n",
    "    ]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement gradient descent with constant step size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent_constant(\n",
    "    cost_function::Function,\n",
    "    gradient::Function,\n",
    "    initial_x::Vector,  # Initial point\n",
    "    alpha,              # Step size\n",
    "    epsilon,            # Termination parameter\n",
    ")\n",
    "    # Initialization\n",
    "    x = initial_x\n",
    "    k = 0\n",
    "    x_history = zeros(Float64, (0, 2))\n",
    "    cost_history = Float64[]\n",
    "    gradient_norm_history = Float64[]\n",
    "\n",
    "    while true\n",
    "        # Find descent direction d\n",
    "        ## TODO: your code here\n",
    "        \n",
    "        # Update history\n",
    "        x_history = vcat(x_history, x')\n",
    "        push!(cost_history, cost_function_val)\n",
    "        push!(gradient_norm_history, gradient_norm)\n",
    "        # Termination condition\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Update x, increment iteration count\n",
    "        ## TODO: your code here\n",
    "\n",
    "    end\n",
    "    return Dict(\n",
    "        \"x\" => x_history,\n",
    "        \"cost\" => cost_history,\n",
    "        \"gradient_norm\" => gradient_norm_history,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can implement gradient descent with exact line search. To do so, we should find $\\alpha_k$ at each iteration depending on $\\bm{x}^k$. Since we have a quadratic optimization problem, the problem $\\min_{\\alpha} f(\\bm{x}^k + \\alpha \\bm{d}^k)$ has the closed-form solution:\n",
    "\\begin{align}\n",
    "    \\alpha_k = \\frac{\n",
    "        (d^k_1)^2 + (d^k_2)^2\n",
    "    }{\n",
    "        2 \\Big( 5 (d^k_1)^2 + (d^k_2)^2 + 4 d^k_1 d^k_2\\Big)\n",
    "    }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent_exact_linesearch(\n",
    "    cost_function::Function,\n",
    "    gradient::Function,\n",
    "    initial_x::Vector,  # Initial point\n",
    "    epsilon,            # Termination parameter\n",
    ")\n",
    "    # Initialization\n",
    "    x = initial_x\n",
    "    k = 0\n",
    "    x_history = zeros(Float64, (0, 2))\n",
    "    cost_history = Float64[]\n",
    "    gradient_norm_history = Float64[]\n",
    "    alpha_history = Float64[]\n",
    "\n",
    "    while true\n",
    "        # Find descent direction d\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Compute step size alpha\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Update history\n",
    "        x_history = vcat(x_history, x')\n",
    "        push!(cost_history, cost_function_val)\n",
    "        push!(gradient_norm_history, gradient_norm)\n",
    "        push!(alpha_history, alpha)\n",
    "\n",
    "        # Termination condition\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Update x, increment iteration count\n",
    "        ## TODO: your code here\n",
    "\n",
    "    end\n",
    "    return Dict(\n",
    "        \"x\" => x_history,\n",
    "        \"cost\" => cost_history,\n",
    "        \"gradient_norm\" => gradient_norm_history,\n",
    "        \"alpha\" => alpha_history,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results of our algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x = [0, 10]\n",
    "epsilon = 1e-5\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_quadratic_exact_linesearch = @time gradient_descent_exact_linesearch(\n",
    "    cost_function_quadratic,\n",
    "    gradient_quadratic,\n",
    "    initial_x, \n",
    "    epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_quadratic_constant = @time gradient_descent_constant(\n",
    "    cost_function_quadratic,\n",
    "    gradient_quadratic,\n",
    "    initial_x, \n",
    "    0.1, \n",
    "    epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of x and y values\n",
    "x_range = -10:0.1:10\n",
    "y_range = -10:0.1:10\n",
    "grid = [(x, y) for x in x_range, y in y_range]\n",
    "\n",
    "z = [cost_function_quadratic([x, y]) for (x, y) in grid]\n",
    "z = reshape(z, length(x_range), length(y_range))'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_quadratic_exact_linesearch = contour(\n",
    "    x_range, y_range, z, \n",
    "    levels = 50, \n",
    "    c = :viridis, color = :auto, \n",
    "    legend = false,\n",
    ")\n",
    "Plots.plot!(\n",
    "    results_quadratic_exact_linesearch[\"x\"][:,1],\n",
    "    results_quadratic_exact_linesearch[\"x\"][:,2],\n",
    "    linestyle = :dash,\n",
    "    linewidth = 2,\n",
    "    markershape = :circle, \n",
    "    color = :red,\n",
    "    title = \"Gradient descent with exact line search ($(length(results_quadratic_exact_linesearch[\"cost\"])) iterations)\"\n",
    ")\n",
    "Plots.savefig(contour_quadratic_exact_linesearch, \"$(@__DIR__)/contour_quadratic_exact_linesearch.png\")\n",
    "contour_quadratic_exact_linesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_quadratic_constant = contour(\n",
    "    x_range, y_range, z, \n",
    "    levels = 50, \n",
    "    c = :viridis, color = :auto, \n",
    "    legend = false,\n",
    ")\n",
    "Plots.plot!(\n",
    "    results_quadratic_constant[\"x\"][:,1],\n",
    "    results_quadratic_constant[\"x\"][:,2],\n",
    "    linestyle = :dash,\n",
    "    linewidth = 2,\n",
    "    markershape = :circle, \n",
    "    color = :red,\n",
    "    title = \"Gradient descent with constant step size ($(length(results_quadratic_constant[\"cost\"])) iterations)\"\n",
    ")\n",
    "Plots.savefig(contour_quadratic_constant, \"$(@__DIR__)/contour_quadratic_constant.png\")\n",
    "contour_quadratic_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results make sense! With exact line search, you make the best possible progress along one direction at each iteration, so there are less iterations. In this particular case, the optimal step size at each iteration can be computed analytically, so the work per iteration is about the same as GD with constant step size. Hence the algorithm runs much faster and consumes less memory with exact line search than with a constant step size.\n",
    "\n",
    "However, for other functions where it's not as easy to solve the argmin problem efficiently, there is a tradeoff between doing more work per iteration and doing less iterations for exact line search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Backtracking line search v.s. constant step size, for a more complicated example\n",
    "\n",
    "What happens when you move beyond quadratic optimization? We minimize the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function), which is often used as a test problem for optimization algorithms:\n",
    "\\begin{align*}   \n",
    "    \\min_{x_1, x_2} f(x_1, x_2) := 100(x_2 - x_1^2)^2 + (1 - x_1)^2\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f(\\bm{x}) = \\begin{pmatrix}\n",
    "        - 400 x_1 (x_2 - x_1^2) - 2 (1 - x_1)\n",
    "        \\\\\n",
    "        200 (x_2 - x_1^2)\n",
    "    \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement backtracking line search for this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function_rosenbrock(x)\n",
    "    return 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2\n",
    "end\n",
    "\n",
    "function gradient_rosenbrock(x)\n",
    "    return [\n",
    "        - 400 * x[1] * (x[2] - x[1]^2) - 2*(1 - x[1]),\n",
    "          200 * (x[2] - x[1]^2),\n",
    "    ]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent_backtracking_linesearch(\n",
    "    cost_function::Function,\n",
    "    gradient::Function,\n",
    "    initial_x::Vector,  # Initial point\n",
    "    epsilon,            # Termination parameter\n",
    "    s,                  # Initial alpha\n",
    "    sigma,              # Backtracking parameter in (0, 0.5]\n",
    "    beta,               # Rate of decrease in alpha in (0, 1)\n",
    ")\n",
    "    # Initialization\n",
    "    x = initial_x\n",
    "    k = 0\n",
    "    x_history = zeros(Float64, (0, 2))\n",
    "    cost_history = Float64[]\n",
    "    gradient_norm_history = Float64[]\n",
    "    alpha_history = Float64[]\n",
    "\n",
    "    while true\n",
    "        # Find descent direction d\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Compute step size alpha\n",
    "        ## TODO: your code here\n",
    "        \n",
    "        # Update history\n",
    "        x_history = vcat(x_history, x')\n",
    "        push!(cost_history, cost_function_val)\n",
    "        push!(gradient_norm_history, gradient_norm)\n",
    "        push!(alpha_history, alpha)\n",
    "        \n",
    "        # Termination condition\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Update x, increment iteration count\n",
    "        ## TODO: your code here\n",
    "\n",
    "    end\n",
    "    return Dict(\n",
    "        \"x\" => x_history,\n",
    "        \"cost\" => cost_history,\n",
    "        \"gradient_norm\" => gradient_norm_history,\n",
    "        \"alpha\" => alpha_history,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x_rosenbrock = [0.25, 4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rosenbrock_constant = @time gradient_descent_constant(\n",
    "    cost_function_rosenbrock,\n",
    "    gradient_rosenbrock,\n",
    "    initial_x_rosenbrock,\n",
    "    0.001,\n",
    "    epsilon,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rosenbrock_backtracking_linesearch = @time gradient_descent_backtracking_linesearch(\n",
    "    cost_function_rosenbrock,\n",
    "    gradient_rosenbrock,\n",
    "    # [2, 5],\n",
    "    initial_x_rosenbrock,\n",
    "    epsilon,\n",
    "    2,      # s\n",
    "    0.25,   # sigma\n",
    "    0.5,    # beta\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of x and y values\n",
    "x_range = 0:0.01:3\n",
    "y_range = 0:0.01:5\n",
    "grid = [(x, y) for x in x_range, y in y_range]\n",
    "\n",
    "z = [cost_function_rosenbrock([x y]) for (x, y) in grid]\n",
    "z = reshape(z, length(x_range), length(y_range))'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_rosenbrock_constant = contour(\n",
    "    x_range, y_range, z, \n",
    "    levels = 1000, \n",
    "    c = :viridis, color = :auto, \n",
    "    legend = false,\n",
    ")\n",
    "Plots.plot!(\n",
    "    results_rosenbrock_constant[\"x\"][:,1],\n",
    "    results_rosenbrock_constant[\"x\"][:,2],\n",
    "    linestyle = :dash,\n",
    "    linewidth = 2,\n",
    "    markershape = :circle, \n",
    "    color = :red,\n",
    "    title = \"Gradient descent with constant step size ($(length(results_rosenbrock_constant[\"cost\"])) iterations)\"\n",
    ")\n",
    "Plots.savefig(contour_rosenbrock_constant, \"$(@__DIR__)/contour_rosenbrock_constant.png\")\n",
    "contour_rosenbrock_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_rosenbrock_backtracking_linesearch = contour(\n",
    "    x_range, y_range, z, \n",
    "    levels = 1000, \n",
    "    c = :viridis, color = :auto, \n",
    "    legend = false,\n",
    ")\n",
    "Plots.plot!(\n",
    "    results_rosenbrock_backtracking_linesearch[\"x\"][:,1],\n",
    "    results_rosenbrock_backtracking_linesearch[\"x\"][:,2],\n",
    "    linestyle = :dash,\n",
    "    linewidth = 2,\n",
    "    markershape = :circle, \n",
    "    color = :red,\n",
    "    title = \"Gradient descent with backtracking_linesearch step size ($(length(results_rosenbrock_backtracking_linesearch[\"cost\"])) iterations)\"\n",
    ")\n",
    "Plots.savefig(contour_rosenbrock_backtracking_linesearch, \"$(@__DIR__)/contour_rosenbrock_backtracking_linesearch.png\")\n",
    "contour_rosenbrock_backtracking_linesearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that backtracking line search is significantly faster than the naive method of GD with a constant step size! Backtracking is also a simple way to approximate exact line search, especially when we don't have a closed form expression for $\\alpha_k$ in exact line search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at how Newton's method performs on the Rosenbrock function above. Recall that Newton's method can potentially converge very quickly to local stationary points, depending on the initialization points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've seen gradient descent on the Rosenbrock function converge to what looks like $(1, 1)$. We first use the `NLsolve.jl` package to find local minima and maxima of the Rosenbrock function, and verify that indeed $(1, 1)$ is a local minimum. Since finding stationary points of a function is equivalent to finding roots of its gradient, we use the gradient function we've defined previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range for initial guesses\n",
    "x_range = 0:0.5:3\n",
    "y_range = 0:0.5:5\n",
    "unique_solutions = Set{Tuple{Float64, Float64}}()\n",
    "for point in Iterators.product(\n",
    "    x_range,\n",
    "    y_range,\n",
    ")\n",
    "    result = nlsolve(gradient_rosenbrock, [point[1], point[2]])\n",
    "    # Convert result to tuple and round to 6 decimal places\n",
    "    # (for numerical accuracy)\n",
    "    solution = (\n",
    "        round(result.zero[1], digits=6), \n",
    "        round(result.zero[2], digits=6),\n",
    "    )\n",
    "    # Add the solution to the set (only unique solutions)\n",
    "    push!(unique_solutions, solution)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, $(1, 1)$ is the only stationary point of the function. If we want to find out if it is a local maximum or minimum, or a saddle point, that depends on whether the eigenvalues of the Hessian at $(1, 1)$ are all negative, all positive, or neither, respectively. We first compute the Hessian:\n",
    "\\begin{align*}\n",
    "    \\nabla^2 f(\\bm{x}) = \\begin{pmatrix}\n",
    "        - 400 (x_2 - x_1^2) + 800 x_1^2 + 2 \n",
    "        & - 400 x_1\n",
    "        \\\\\n",
    "        - 400 x_1 \n",
    "        & 200\n",
    "    \\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function hessian_rosenbrock(x::Vector)\n",
    "    d2f_dx1dx1 = - 400 * (x[2] - x[1]^2)^2 + 800 * x[1]^2 + 2\n",
    "    d2f_dx1dx2 = - 400 * x[1]\n",
    "    d2f_dx2dx2 = 200\n",
    "    return [d2f_dx1dx1 d2f_dx1dx2; d2f_dx1dx2 d2f_dx2dx2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = eigen(hessian_rosenbrock([1, 1])).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the eigenvalues are all positive, $(1, 1)$ is indeed a local minima. (How can you show that $(1, 1)$ is a global minimum?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement Newton's method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newtons_method(\n",
    "    cost_function::Function,\n",
    "    gradient::Function,\n",
    "    hessian::Function,\n",
    "    initial_x::Vector,          # Initial point\n",
    "    ;\n",
    "    epsilon::Float64 = 1e-6,    # Termination parameter\n",
    "    max_iters::Int = 1000,\n",
    ")\n",
    "    # Initialization\n",
    "    x = initial_x\n",
    "    k = 0\n",
    "    x_history = zeros(Float64, (0, 2))\n",
    "    cost_history = Float64[]\n",
    "\n",
    "    while k < max_iters\n",
    "        # Find descent direction d\n",
    "        ## TODO: your code here\n",
    "        # Update history\n",
    "        x_history = vcat(x_history, x')\n",
    "        push!(cost_history, cost_function_val)\n",
    "\n",
    "        # Termination condition\n",
    "        ## TODO: your code here\n",
    "\n",
    "        # Update x, increment iteration count\n",
    "        ## TODO: your code here\n",
    "\n",
    "    end\n",
    "    return Dict(\n",
    "        \"x\" => x_history,\n",
    "        \"cost\" => cost_history,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rosenbrock_newtons_method = newtons_method(\n",
    "    cost_function_rosenbrock,\n",
    "    gradient_rosenbrock,\n",
    "    hessian_rosenbrock,\n",
    "    initial_x_rosenbrock,\n",
    "    ;\n",
    "    max_iters = 100, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of x and y values\n",
    "x_range = 0:0.01:3\n",
    "y_range = 0:0.01:5\n",
    "grid = [(x, y) for x in x_range, y in y_range]\n",
    "\n",
    "z = [cost_function_rosenbrock([x y]) for (x, y) in grid]\n",
    "z = reshape(z, length(x_range), length(y_range))'\n",
    "\n",
    "contour_rosenbrock_newtons_method = contour(\n",
    "    x_range, y_range, z, \n",
    "    levels = 1000, \n",
    "    c = :viridis, color = :auto, \n",
    "    legend = false,\n",
    ")\n",
    "Plots.plot!(\n",
    "    results_rosenbrock_newtons_method[\"x\"][:,1],\n",
    "    results_rosenbrock_newtons_method[\"x\"][:,2],\n",
    "    linestyle = :dash,\n",
    "    linewidth = 2,\n",
    "    markershape = :circle, \n",
    "    color = :red,\n",
    "    title = \"Newton's method with alpha = 1 ($(length(results_rosenbrock_newtons_method[\"cost\"])) iterations)\"\n",
    ")\n",
    "Plots.savefig(contour_rosenbrock_newtons_method, \"$(@__DIR__)/contour_rosenbrock_newtons_method.png\")\n",
    "contour_rosenbrock_newtons_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool! Newton's method works very well on Rosenbrock's function. Do try other initialization points and see what happens. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
